import fs from "node:fs";
import path from "node:path";
import { fileURLToPath } from "node:url";
import { chromium } from "playwright";
import pLimit from "p-limit";
import BRANDS from "./selectors.config.js";

// ───────────────────────────────────────────────────────────
// Config
// ───────────────────────────────────────────────────────────
const APPS_SCRIPT_WEBHOOK = process.env.APPS_SCRIPT_WEBHOOK || "";
const JOB_TOKEN = process.env.JOB_TOKEN || "";
const PROXY_URL = process.env.PROXY_URL || ""; // set in GitHub Secrets

// Crawl limits (safe defaults)
const HARD_MAX_PAGES = Number(process.env.HARD_MAX_PAGES || 300);
const PAGE_TIMEOUT_MS = 90_000;
const GRID_WAIT_MS = 20_000;
const LIST_CONCURRENCY = 2;

// Early-stop tuning:
// - Stop after this many consecutive pages with < MIN_ITEMS_PER_PAGE links.
const MIN_ITEMS_PER_PAGE = 6;
const LOW_ITEMS_STREAK_TO_STOP = 3;

// ───────────────────────────────────────────────────────────
// Helpers
// ───────────────────────────────────────────────────────────
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

function log(...args) {
  console.log(...args);
}

async function postRows(rows) {
  if (!APPS_SCRIPT_WEBHOOK) {
    log("[post] APPS_SCRIPT_WEBHOOK missing; skipping post.");
    return { ok: false, skipped: true };
  }
  const payload = {
    token: JOB_TOKEN || "",
    rows
  };

  const res = await fetch(APPS_SCRIPT_WEBHOOK, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(payload)
  });

  const text = await res.text().catch(() => "");
  let data = {};
  try {
    data = JSON.parse(text);
  } catch {
    // leave as text
  }
  log(`[post] status ${res.status} | response:`, data || text);
  return { ok: res.ok, data: data || text };
}

function ensureArtifactsDir() {
  const dir = path.join(__dirname);
  return dir;
}

async function saveArtifact(prefix, pageNum, page) {
  const base = ensureArtifactsDir();
  const png = path.join(base, `${prefix}-p${pageNum}.png`);
  const html = path.join(base, `${prefix}-p${pageNum}.html`);
  await page.screenshot({ path: png, fullPage: true }).catch(() => {});
  const content = await page.content().catch(() => "<!-- no content -->");
  fs.writeFileSync(html, content);
}

// ───────────────────────────────────────────────────────────
// Core crawler
// ───────────────────────────────────────────────────────────
async function crawlBrand(brandName, shardFrom = 1, shardTo = HARD_MAX_PAGES) {
  const brand = BRANDS[brandName];
  if (!brand) throw new Error(`Unknown brand: ${brandName}`);

  const launchOpts = {
    headless: true,
    timeout: PAGE_TIMEOUT_MS
  };

  if (PROXY_URL) {
    log(`[${brandName}] Using proxy: ***`);
    launchOpts.proxy = { server: PROXY_URL };
  }

  const browser = await chromium.launch(launchOpts);
  const context = await browser.newContext({
    userAgent:
      "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
  });
  const page = await context.newPage();

  let totalLinks = 0;
  let lowItemsStreak = 0;

  for (let p = shardFrom; p <= shardTo && p <= HARD_MAX_PAGES; p++) {
    const listUrl = brand.urlForPage(p);
    log(`[${brandName}] Visiting ${listUrl}`);

    try {
      await page.goto(listUrl, { waitUntil: "domcontentloaded", timeout: PAGE_TIMEOUT_MS });

      // Wait for the grid/list to appear (or time out gently)
      const waitPromise = page.waitForSelector(brand.waitFor, { timeout: GRID_WAIT_MS });
      await waitPromise.catch(() => {});

      // Extract links
      const links = await page.$$eval(brand.linkSelector, (nodes) => {
        const uniq = new Set();
        nodes.forEach((n) => {
          const href = n.getAttribute("href") || "";
          if (!href) return;
          // normalize absolute
          let url = href;
          if (href.startsWith("/")) {
            const origin = location.origin;
            url = origin + href;
          }
          uniq.add(url);
        });
        return Array.from(uniq);
      }).catch(() => []);

      const items = links.length;
      totalLinks += items;

      // Save artifacts for debugging
      const key = brandName.toLowerCase();
      await saveArtifact(key, p, page);

      log(`[${brandName}] page ${p} => items: ${items}`);

      if (items === 0) {
        log(`[${brandName}] No links found on page ${p}, stopping.`);
        break;
      }

      if (items < MIN_ITEMS_PER_PAGE) {
        lowItemsStreak += 1;
        if (lowItemsStreak >= LOW_ITEMS_STREAK_TO_STOP) {
          log(
            `[${brandName}] ${LOW_ITEMS_STREAK_TO_STOP} low-yield pages in a row (< ${MIN_ITEMS_PER_PAGE}). Stopping.`
          );
          break;
        }
      } else {
        lowItemsStreak = 0;
      }

      // Prepare rows to post
      const rows = links.map((href) => {
        let name = "";
        let city = "";
        // Cheap name guess from URL path
        try {
          const u = new URL(href);
          const last = u.pathname.split("/").filter(Boolean).pop() || "";
          name = decodeURIComponent(last.replace(/[-_]+/g, " ")).trim();
        } catch {}
        return {
          source: brandName,
          dirUrl: listUrl,
          href,
          name,
          city,
          fetchedAt: new Date().toISOString()
        };
      });

      if (rows.length > 0) {
        await postRows(rows).catch((e) => log("[postRows] error:", e.message));
      }
    } catch (err) {
      const msg = String(err && err.message ? err.message : err);
      log(`[${brandName}] page ${p} navigation error: ${msg}`);
      // If we repeatedly time out, bail out to avoid burning minutes.
      if (msg.includes("Timeout")) break;
    }
  }

  log(`[${brandName}] Total links found: ${totalLinks}`);
  await browser.close();
}

// ───────────────────────────────────────────────────────────
// Entry
// ───────────────────────────────────────────────────────────
async function main() {
  const brand = process.env.BRAND || "TagVenue";
  const shardFrom = Number(process.env.SHARD_FROM || 1);
  const shardTo = Number(process.env.SHARD_TO || HARD_MAX_PAGES);

  const limiter = pLimit(LIST_CONCURRENCY);

  // Only one brand per job in our workflow, but keep the limiter pattern
  await limiter(() => crawlBrand(brand, shardFrom, shardTo));
}

main().catch((e) => {
  console.error(e);
  process.exit(1);
});
